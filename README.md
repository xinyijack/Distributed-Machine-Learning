# Distributed-Machine-Learning
##同步算法
###ASGD  
优化方法：
考虑全局与本地差异
添加冲量（历史参数对下一次参数的影响）
添加正则项（SVRG方差缩减）

##异步算法
延迟处理————步长与延迟τ的影响
近似处理————泰勒展开零阶一阶项近似下一状态梯度
正则项引入————SVRG、SAG、SAGA
